{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"OurCodewithOurData: Transformer pipeline with preprocessing, plots, and accuracy metrics\"\"\"\n",
    "\n",
    "!pip install torch matplotlib pandas scikit-learn\n",
    "\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# --- Dataset ---------------------------------------------------------------\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, series, input_len, pred_len):\n",
    "        \"\"\"\n",
    "        series: np.ndarray of shape (N, num_features)\n",
    "        \"\"\"\n",
    "        self.series = series\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.total_samples = len(self.series) - self.input_len - self.pred_len + 1\n",
    "        logger.info(\n",
    "            f\"TimeSeriesDataset init: series_shape={series.shape}, \"\n",
    "            f\"input_len={input_len}, pred_len={pred_len}, total_samples={self.total_samples}\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.series[idx : idx + self.input_len]\n",
    "        y = self.series[idx + self.input_len : idx + self.input_len + self.pred_len]\n",
    "        return (\n",
    "            torch.tensor(x, dtype=torch.float32, device=device),\n",
    "            torch.tensor(y, dtype=torch.float32, device=device)\n",
    "        )\n",
    "\n",
    "# --- Transformer Components ------------------------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=40000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, device=device)\n",
    "        pos = torch.arange(0, max_len, device=device).unsqueeze(1).float()\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2, device=device).float() * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.Wq = nn.Linear(d_model, d_model).to(device)\n",
    "        self.Wk = nn.Linear(d_model, d_model).to(device)\n",
    "        self.Wv = nn.Linear(d_model, d_model).to(device)\n",
    "        self.Wo = nn.Linear(d_model, d_model).to(device)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        B = q.size(1)\n",
    "        Q = self.Wq(q); K = self.Wk(k); V = self.Wv(v)\n",
    "        Qh = Q.view(-1, B, self.num_heads, self.d_k).transpose(1,2)\n",
    "        Kh = K.view(-1, B, self.num_heads, self.d_k).transpose(1,2)\n",
    "        Vh = V.view(-1, B, self.num_heads, self.d_k).transpose(1,2)\n",
    "        scores = torch.matmul(Qh, Kh.transpose(-2,-1)) / self.scale\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        ctx  = torch.matmul(attn, Vh)\n",
    "        ctx2 = ctx.transpose(1,2).contiguous().view(-1, B, self.num_heads*self.d_k)\n",
    "        return self.Wo(ctx2)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedfwd=4096, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model).to(device)\n",
    "        self.ff    = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedfwd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedfwd, d_model)\n",
    "        ).to(device)\n",
    "        self.norm2 = nn.LayerNorm(d_model).to(device)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a  = self.attn(x, x, x)\n",
    "        x2 = self.norm1(x + self.drop(a))\n",
    "        f  = self.ff(x2)\n",
    "        return self.norm2(x2 + self.drop(f))\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, d_model, num_heads, num_layers, pred_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model).to(device)\n",
    "        self.pos_enc    = PositionalEncoding(d_model)\n",
    "        self.layers     = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.pred_len   = pred_len\n",
    "        self.output_proj= nn.Linear(d_model, input_size).to(device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        x = self.input_proj(src)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_proj(x[-self.pred_len:])\n",
    "\n",
    "# --- Training & Evaluation ------------------------------------------------\n",
    "def train(model, loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in loader:\n",
    "            x = xb.permute(1, 0, 2)\n",
    "            y = yb.permute(1, 0, 2)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        logger.info(f\"Epoch {epoch}: avg loss {epoch_loss/len(loader):.6f}\")\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    preds_all, actual_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            x = xb.permute(1, 0, 2)\n",
    "            y = yb.permute(1, 0, 2)\n",
    "            preds = model(x).cpu().numpy()\n",
    "            preds_all.append(preds)\n",
    "            actual_all.append(y.cpu().numpy())\n",
    "    preds_all = np.concatenate(preds_all, axis=1).reshape(-1, preds_all[0].shape[2])\n",
    "    actual_all = np.concatenate(actual_all, axis=1).reshape(-1, actual_all[0].shape[2])\n",
    "    mse = mean_squared_error(actual_all, preds_all)\n",
    "    mae = mean_absolute_error(actual_all, preds_all)\n",
    "    mape = mean_absolute_percentage_error(actual_all, preds_all)\n",
    "    r2 = r2_score(actual_all, preds_all)\n",
    "    logger.info(f\"Eval MSE: {mse:.6f}, MAE: {mae:.6f}, MAPE: {mape:.6f}, R2: {r2:.6f}\")\n",
    "    print(f\"Test Metrics -> MSE: {mse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2%}, R2: {r2:.4f}\")\n",
    "    return preds_all, actual_all\n",
    "\n",
    "# ------------------ Setup & Data -------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "fh = logging.FileHandler('transformer_tutorial.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Data Loading & Preprocessing ---------------------------------------------\n",
    "csv_path = '/content/drive/MyDrive/Theis/Thesis/train.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "if '5 Minutes' in df.columns:\n",
    "    df.rename(columns={'5 Minutes': 'DateTime'}, inplace=True)\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'], dayfirst=True, infer_datetime_format=True)\n",
    "df.set_index('DateTime', inplace=True)\n",
    "df = df.drop(['datetime', 'Unnamed: 0', 'day_of_week', '# Lane Points', '% Observed'], axis=1, errors='ignore')\n",
    "logger.info(f\"Loaded and cleaned data: {df.shape}\")\n",
    "\n",
    "# Raw Data Plot -------------------------------------------------------------\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df.index, df.iloc[:,0])\n",
    "plt.title(df.columns[0])\n",
    "plt.xlabel('DateTime')\n",
    "plt.ylabel(df.columns[0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Daily Mean Resample Plot -------------------------------------------------\n",
    "daily_mean = df.resample('D').mean()\n",
    "plt.figure(figsize=(12,6))\n",
    "for col in daily_mean.columns:\n",
    "    plt.plot(daily_mean.index, daily_mean[col], label=col)\n",
    "plt.title('Daily Mean of ' + ', '.join(daily_mean.columns))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mean Value')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Train/Test Split & Loaders ------------------------------------------------\n",
    "input_len, pred_len, train_ratio = 30, 20, 0.8\n",
    "total_len = len(df)\n",
    "split_idx = int(total_len * train_ratio)\n",
    "data_np = df.values.astype(np.float32)\n",
    "\n",
    "train_series = data_np[:split_idx]\n",
    "test_series = data_np[split_idx - input_len:]\n",
    "\n",
    "train_ds = TimeSeriesDataset(train_series, input_len, pred_len)\n",
    "test_ds = TimeSeriesDataset(test_series, input_len, pred_len)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=32)\n",
    "\n",
    "# Model, Loss, Optimizer ----------------------------------------------------\n",
    "input_size = df.shape[1]\n",
    "model = TransformerModel(input_size, d_model=64, num_heads=8, num_layers=2, pred_len=pred_len).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training ------------------------------------------------------------------\n",
    "train(model, train_loader, optimizer, criterion)\n",
    "\n",
    "# Evaluation & Accuracy Metrics --------------------------------------------\n",
    "preds_all, actual_all = evaluate(model, test_loader, criterion)\n",
    "\n",
    "# Final Forecast & Plotting ------------------------------------------------\n",
    "window = test_series[-input_len:]\n",
    "win_t = torch.tensor(window, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(win_t).squeeze(1).cpu().numpy()\n",
    "actual = df.iloc[split_idx:split_idx+pred_len].values\n",
    "\n",
    "dates = df.index[split_idx:split_idx+pred_len]\n",
    "plt.figure(figsize=(12, 2*input_size))\n",
    "for i, col in enumerate(df.columns):\n",
    "    ax = plt.subplot(input_size, 1, i+1)\n",
    "    ax.plot(dates, actual[:, i], label='Actual')\n",
    "    ax.plot(dates, preds[:, i], label='Predicted')\n",
    "    ax.set_title(col)\n",
    "    if i == input_size - 1: ax.set_xlabel('Date')\n",
    "    ax.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute accuracy for final forecast\n",
    "mse_f=mean_squared_error(actual, preds)\n",
    "mae_f=mean_absolute_error(actual, preds)\n",
    "mape_f=mean_absolute_percentage_error(actual, preds)\n",
    "r2_f=r2_score(actual, preds)\n",
    "print(f\"Final Forecast Metrics -> MSE:{mse_f:.4f}, MAE:{mae_f:.4f}, MAPE:{mape_f:.2%}, R2:{r2_f:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
